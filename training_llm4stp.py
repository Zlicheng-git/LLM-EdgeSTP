# -*- coding: utf-8 -*-
"""
Created on Wed Dec 10 15:33:23 2025

@author: HP
"""

import torch
import torch.nn as nn
#import torch.optim as optim
#from torch.amp import GradScaler
import numpy as np
from tqdm import tqdm
import os
import pandas as pd
#import gc
#from codecarbon import EmissionsTracker
from modeling_llm4stp import LLM4STPModel
from Config_llm4stp import LLM4STPConfig
from data import DataProcessor
# from codecarbon import EmissionsTracker

# è¯„ä¼°æŒ‡æ ‡
def haversine_distance(y_true, y_pred):
    lon1, lat1 = y_true[..., 0], y_true[..., 1]
    lon2, lat2 = y_pred[..., 0], y_pred[..., 1]
    
    lon1_rad = lon1 * (np.pi / 180.0)
    lat1_rad = lat1 * (np.pi / 180.0)
    lon2_rad = lon2 * (np.pi / 180.0)
    lat2_rad = lat2 * (np.pi / 180.0)
    
    dlon = lon2_rad - lon1_rad
    dlat = lat2_rad - lat1_rad
    
    a = torch.sin(dlat / 2.0) ** 2 + \
        torch.cos(lat1_rad) * torch.cos(lat2_rad) * torch.sin(dlon / 2.0) ** 2
    c = 2 * torch.asin(torch.sqrt(a))
    
    distance = c * 6371000  # å•ä½ï¼šç±³
    return distance

def haversine_FDE(y_true: torch.Tensor, y_pred: torch.Tensor, lat_min, lat_max, lon_min, lon_max) -> torch.Tensor:
    assert y_true.dim() == 2 and y_pred.dim() == 2, "è¾“å…¥å¿…é¡»æ˜¯2D: [batch_size, seq_len * 2]"
    assert y_true.shape == y_pred.shape, "y_trueå’Œy_predå¿…é¡»å½¢çŠ¶ç›¸åŒ"
    assert y_true.shape[1] % 2 == 0, "æœ€åä¸€ç»´å¿…é¡»æ˜¯å¶æ•°"

    seq_len = y_true.shape[1] // 2
    y_true_reshaped = y_true.view(-1, seq_len, 2)
    y_pred_reshaped = y_pred.view(-1, seq_len, 2)

    y_true_last = y_true_reshaped[:, -1, :]
    y_pred_last = y_pred_reshaped[:, -1, :]

    y_true_denorm = min_max_denormalize(y_true_last, lat_min, lat_max, lon_min, lon_max)
    y_pred_denorm = min_max_denormalize(y_pred_last, lat_min, lat_max, lon_min, lon_max)

    distances = haversine_distance(y_true_denorm, y_pred_denorm)
    return distances.mean()

def haversine_ADE(y_true: torch.Tensor, y_pred: torch.Tensor, lat_min, lat_max, lon_min, lon_max,
                 sample_weight: torch.Tensor = None) -> torch.Tensor:
    assert y_true.dim() == 2 and y_pred.dim() == 2, "è¾“å…¥å¿…é¡»æ˜¯2D: [batch_size, seq_len * 2]"
    assert y_true.shape == y_pred.shape, "y_trueå’Œy_predå¿…é¡»å½¢çŠ¶ç›¸åŒ"
    assert y_true.shape[1] % 2 == 0, "æœ€åä¸€ç»´å¿…é¡»æ˜¯å¶æ•°"

    seq_len = y_true.shape[1] // 2
    y_true_reshaped = y_true.view(-1, seq_len, 2)
    y_pred_reshaped = y_pred.view(-1, seq_len, 2)

    y_true_denorm = min_max_denormalize(y_true_reshaped, lat_min, lat_max, lon_min, lon_max)
    y_pred_denorm = min_max_denormalize(y_pred_reshaped, lat_min, lat_max, lon_min, lon_max)

    distances = haversine_distance(y_true_denorm, y_pred_denorm)
    mean_distances_per_sample = distances.mean(dim=-1)

    if sample_weight is not None:
        sample_weight = sample_weight.float().to(mean_distances_per_sample.device)
        return (mean_distances_per_sample * sample_weight).sum() / sample_weight.sum()
    else:
        return mean_distances_per_sample.mean()

def min_max_denormalize(normalize_data, lat_min, lat_max, lon_min, lon_max):
    normalized_lon, normalized_lat = normalize_data[..., 0], normalize_data[..., 1]
    lon = normalized_lon * (lon_max - lon_min) + lon_min
    lat = normalized_lat * (lat_max - lat_min) + lat_min
    return torch.stack([lon, lat], dim=-1)

def mean_absolute_error(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def format_num(num):
    """è‡ªå®šä¹‰æ•°å€¼æ ¼å¼åŒ–ï¼šå°äº0.0001ç”¨ç§‘å­¦è®¡æ•°æ³•ï¼Œå¦åˆ™ä¿ç•™4ä½å°æ•°"""
    if abs(num) < 1:  # ç»å¯¹å€¼å°äº0.0001ï¼Œç”¨ç§‘å­¦è®¡æ•°æ³•
        return f"{num:.4e}"
    else:  # å¦åˆ™ä¿ç•™4ä½å°æ•°ï¼ˆè‡ªåŠ¨å»é™¤æœ«å°¾å¤šä½™0ï¼‰
        return f"{num:.4f}".rstrip('0').rstrip('.') if '.' in f"{num:.4f}" else f"{num:.4f}"

if __name__ == "__main__":
    config = LLM4STPConfig()
    config.area = 'osaka'
    config.model_name = "/root/autodl-tmp/Gemma3-1B"
    config.learning_rate = 0.00001
    config.batch_size = 10
    
    
    
    # # åˆå§‹åŒ–è®°å½•æŒ‡æ ‡çš„æ•°æ®ç»“æ„
    processor = DataProcessor(
        data_path='/root/autodl-tmp/llm4stp_data/featuer_AISCN_02-06_daban_202565.csv',
        region = config.area,
        use_cols= ['UnixTime_FEN', 'MMSI_', 'Course', 'Speed', 'Lon_d', 'Lat_d',
            'df0_Course', 'df0_Speed', 'df0_Lon_d', 'df0_Lat_d',
            'df1_Course', 'df1_Speed', 'df1_Lon_d', 'df1_Lat_d',
        ]
    )
    processor.process(input_len=config.seq_len, output_len=config.pred_len, train_ratio=0.8)

    # ç„¶åè·å– DataLoader , train_num_samples
    train_loader, test_loader = processor.get_dataloaders(batch_size=config.batch_size, shuffle=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    
    model = LLM4STPModel(config)
    model.to(device)
    need_add = False
    # ä¼˜åŒ–å™¨è®¾ç½®ï¼ˆä¿æŒä¸å˜ï¼‰
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)

    metrics = {
        'train': {
            'train_mae': [], 'train_mse': [], 'train_ade': [], 'train_fde': [],
        },
        'val': {
            'val_mae': [], 'val_mse': [], 'val_ade': [], 'val_fde': [], 'lr': [],# 'CO2': [],
        }
    }
    
    regression_loss_fn = nn.MSELoss()
    scheduler = torch.optim.lr_scheduler.ExponentialLR(
        optimizer=optimizer, gamma=0.99)
    
    
    
    epochs = 500
    for epoch in range(epochs):
        print(f"\nEpoch {epoch + 1}/{epochs}")
        # tracker.start()
        model.train()
        total_train_mae_loss = 0
        total_train_mse_loss = 0
        train_total = 0
        train_ade = 0.0
        train_fde = 0.0
        
        for step, batch in enumerate(tqdm(train_loader)):
            input_ids = batch[0].to(device)
            # print(input_ids.shape, "input_ids")
            labels = batch[-1].to(device)
            batch_size = labels.size(0)
            train_total += batch_size
            
            optimizer.zero_grad()            
            pred_output = model(input_ids)
            # æŸå¤±è®¡ç®—ä¹Ÿè‡ªåŠ¨åœ¨åŠç²¾åº¦ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œ
            mae_loss = mean_absolute_error(pred_output, labels)
            mse_loss = regression_loss_fn(pred_output, labels)
            
            # è®¡ç®—ADEå’ŒFDE
            ade = haversine_ADE(labels, pred_output, config.REGION_CONFIGS['osaka']['lat_min']
                                , config.REGION_CONFIGS['osaka']['lat_max']
                                , config.REGION_CONFIGS['osaka']['lon_min']
                                , config.REGION_CONFIGS['osaka']['lon_max'])
            fde = haversine_FDE(labels, pred_output, config.REGION_CONFIGS['osaka']['lat_min']
                                , config.REGION_CONFIGS['osaka']['lat_max']
                                , config.REGION_CONFIGS['osaka']['lon_min']
                                , config.REGION_CONFIGS['osaka']['lon_max'])
            

            # ç´¯åŠ æŸå¤±
            total_train_mae_loss += mae_loss.item() * batch_size
            total_train_mse_loss += mse_loss.item() * batch_size
            train_ade += ade.item() * batch_size
            train_fde += fde.item() * batch_size

            mse_loss.backward()
            optimizer.step()
            # scheduler.step()

        # è®¡ç®—è®­ç»ƒé›†å¹³å‡æŒ‡æ ‡
        avg_train_mae_loss = total_train_mae_loss / train_total if train_total > 0 else 0
        avg_train_mse_loss = total_train_mse_loss / train_total if train_total > 0 else 0
        avg_train_ade = train_ade / train_total if train_total > 0 else 0
        avg_train_fde = train_fde / train_total if train_total > 0 else 0
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        metrics['train']['train_mae'].append(avg_train_mae_loss)
        metrics['train']['train_mse'].append(avg_train_mse_loss)
        metrics['train']['train_ade'].append(avg_train_ade)
        metrics['train']['train_fde'].append(avg_train_fde)

        # step_co2 = tracker.stop()
        # step_co2_list.append(step_co2)
        # è¯„ä¼°é˜¶æ®µ
        model.eval()
        total_eval_mae_loss = 0
        total_eval_mse_loss = 0
        eval_total = 0
        eval_ade = 0.0
        eval_fde = 0.0
       
        
        if len(test_loader) == 0:
            print("è­¦å‘Š: æµ‹è¯•é›†ä¸ºç©ºï¼Œè·³è¿‡è¯„ä¼°")
            for key in metrics['val']:
                metrics['val'][key].append(np.nan)
            # æ¯ä¸ªepochç»“æŸåæ¸…ç†ç¼“å­˜
            if device.type == 'cuda':
                torch.cuda.empty_cache()
            continue
        #tracker = EmissionsTracker(
        #     # output_dir=output_dir,
        #     # offline=True,
        #    log_level="error",
        #    save_to_file=False)
        
        #tracker.start()
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Evaluating"):
                input_ids = batch[0].to(device)
                labels = batch[-1].to(device)
                batch_size = labels.size(0)
                eval_total += batch_size
                
                pred_output = model(input_ids)
                
                # step_co2_list.append(step_co2)
                # æŸå¤±è®¡ç®—ä¹Ÿè‡ªåŠ¨åœ¨åŠç²¾åº¦ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œ
                mae_loss = mean_absolute_error(pred_output, labels)
                mse_loss = regression_loss_fn(pred_output, labels)
                
                ade = haversine_ADE(labels, pred_output, config.REGION_CONFIGS['osaka']['lat_min']
                                    , config.REGION_CONFIGS['osaka']['lat_max']
                                    , config.REGION_CONFIGS['osaka']['lon_min']
                                    , config.REGION_CONFIGS['osaka']['lon_max'])
                fde = haversine_FDE(labels, pred_output, config.REGION_CONFIGS['osaka']['lat_min']
                                    , config.REGION_CONFIGS['osaka']['lat_max']
                                    , config.REGION_CONFIGS['osaka']['lon_min']
                                    , config.REGION_CONFIGS['osaka']['lon_max'])
                
                # ç´¯åŠ æŸå¤±
                total_eval_mae_loss += mae_loss.item() * batch_size
                total_eval_mse_loss += mse_loss.item() * batch_size
                eval_ade += ade.item() * batch_size
                eval_fde += fde.item() * batch_size
        #step_co2 = tracker.stop()
        # è®¡ç®—éªŒè¯é›†å¹³å‡æŒ‡æ ‡
        avg_eval_mae_loss = total_eval_mae_loss / eval_total if eval_total > 0 else 0
        avg_eval_mse_loss = total_eval_mse_loss / eval_total if eval_total > 0 else 0
        avg_eval_ade = eval_ade / eval_total if train_total > 0 else 0
        avg_eval_fde = eval_fde / eval_total if train_total > 0 else 0
        
        # è®°å½•éªŒè¯æŒ‡æ ‡
        metrics['val']['val_mae'].append(avg_eval_mae_loss)
        metrics['val']['val_mse'].append(avg_eval_mse_loss)
        metrics['val']['val_ade'].append(avg_eval_ade)
        metrics['val']['val_fde'].append(avg_eval_fde)
        metrics['val']['lr'].append(scheduler.get_lr())
        #metrics['val']['CO2'].append(step_co2)
       
        # print(f"\nEpoch {epoch + 1}/{epochs}")
        print(f"Epoch {epoch + 1}-----"
              f"Train_MSE: {format_num(avg_train_mse_loss)}, Train_MAE: {avg_train_mae_loss:.4f}, Train_ADE: {avg_train_ade:.4f}, Train_FDE: {avg_train_fde:.4f}, "
              #f"CO2: {step_co2 * 1e6:.2f} mg, "
              f"Val_MSE: {format_num(avg_eval_mse_loss)}, Val_MAE: {avg_eval_mae_loss:.4f}, Val_ADE: {avg_eval_ade:.4f}, Val_FDE: {avg_eval_fde:.4f}, "
              )
        
        #scheduler.step()
        print(scheduler.get_last_lr())

        # # # æ¯ä¸ªepochç»“æŸåæ¸…ç†CUDAç¼“å­˜
        # if device.type == 'cuda':
        #     torch.cuda.empty_cache()
    save_metrics = pd.concat([pd.DataFrame(metrics['train']), pd.DataFrame(metrics['val'])], axis=1)
    save_metrics.to_csv("/root/autodl-tmp/llm4stp_gemma/STP2LLM_gemma3-1B.csv")
    
    temp_dir = "/root/autodl-tmp/llm4stp_qwen/safetensors_temp"
    os.makedirs(temp_dir, exist_ok=True)
    os.environ["SAFETENSORS_TEMP_DIR"] = temp_dir  # safetensorsä¸“å±ä¸´æ—¶ç›®å½•
    os.environ["TMPDIR"] = temp_dir                # ç³»ç»Ÿä¸´æ—¶ç›®å½•
    os.environ["TEMP"] = temp_dir                  # Windowsä¸´æ—¶ç›®å½•
    os.environ["TMP"] = temp_dir                   # Windows TMPç›®å½•

    # åˆå§‹åŒ–æ¨¡å‹
    # config = LLM4STPConfig()
    # model = STPLLMModel(config)
    # processor = ProcessorLLM4STP(config)
    # processor.process()
    print(f"âœ… æ¨¡å‹å‚æ•°æ€»é‡ï¼š{sum(p.numel() for p in model.parameters()) / 1024**3:.2f} GB")

    # ä¿å­˜ç›®å½•ï¼ˆDç›˜ï¼Œç©ºé—´å……è¶³ï¼‰
    save_dir = "/root/autodl-tmp/llm4stp_qwen/model_llm4stp"
    os.makedirs(save_dir, exist_ok=True)
    
    config.save_pretrained(save_dir)
    
    print("12")
    # æ ¸å¿ƒï¼šå³ä½¿å¼€å¯safe_serializationä¹Ÿèƒ½ä¿å­˜
    model.save_pretrained(
        save_dir,
        safe_serialization=True,  # æ¢å¤safetensorsï¼ˆå¯é€‰ï¼‰
        # max_shard_size="10GB",    # åˆ†ç‰‡ï¼ˆå¯é€‰ï¼‰
        temp_dir=temp_dir         # æ˜¾å¼æŒ‡å®šä¸´æ—¶ç›®å½•ï¼ˆåŒé‡ä¿éšœï¼‰
    )

    print("13")
    model.save_pretrained(
        save_dir,
        safe_serialization=False,  # ä¿å­˜ä¸ºptæ ¼å¼ï¼ˆå¦‚éœ€safetensorsåˆ™è®¾ä¸ºTrueï¼‰
        # max_shard_size="10GB",      # æ¯ä¸ªåˆ†ç‰‡æœ€å¤§2GBï¼ˆæŒ‰éœ€è°ƒæ•´ï¼š1GB/5GB/10GBï¼‰
        save_function=torch.save   # æ˜¾å¼æŒ‡å®šä¿å­˜å‡½æ•°
    )

    # éªŒè¯ä¿å­˜ç»“æœ
    print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸï¼")
    print(f"ä¿å­˜ç›®å½•ï¼š{save_dir}")
    print(f"æ–‡ä»¶åˆ—è¡¨ï¼š{os.listdir(save_dir)}")
    # processor.save_pretrained(save_dir)
    print("processor save!")
    # processor = AutoProcessor.from_pretrained("llm4stp", data_path=save_dir)
    # ========== 5. ä¿å­˜åˆ†è¯å™¨ï¼ˆæ ¸å¿ƒï¼šç”Ÿæˆæ‰€æœ‰tokenizerç›¸å…³æ–‡ä»¶ï¼‰ ==========
    """
    å¯é€‰ä¸¤ç§æ–¹å¼ï¼š
    æ–¹å¼1ï¼šåŸºäºå·²æœ‰å¼€æºåˆ†è¯å™¨ï¼ˆå¦‚LLaMA/Qwen/GLMï¼‰
    æ–¹å¼2ï¼šè‡ªå®šä¹‰åˆ†è¯å™¨ï¼ˆä»0æ„å»ºï¼‰
    """
    # ------------------- æ–¹å¼1ï¼šåŸºäºå¼€æºåˆ†è¯å™¨ï¼ˆæ¨èï¼Œå¯¹é½LLMï¼‰ -------------------
    # åŠ è½½å¼€æºåˆ†è¯å™¨ï¼ˆä»¥Qwen-7Bä¸ºä¾‹ï¼Œæ›¿æ¢ä¸ºä½ çš„LLMå¯¹åº”åˆ†è¯å™¨ï¼‰
    # tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B")

    # print("14")
    # # é€‚é…è‡ªå®šä¹‰æ¨¡å‹çš„ç‰¹æ®ŠTokenï¼ˆå¿…é¡»ä¸configä¸­çš„token_idä¸€è‡´ï¼‰
    # tokenizer.bos_token_id = config.bos_token_id
    # tokenizer.eos_token_id = config.eos_token_id
    # tokenizer.pad_token_id = config.pad_token_id
    tokenizer = model.tokenizer

    tokenizer.save_pretrained(save_dir)

    # ========== 6. éªŒè¯æœ€ç»ˆæ–‡ä»¶ç»“æ„ï¼ˆå®Œæ•´HFæ ‡å‡†ï¼‰ ==========
    print("\nğŸ“ æœ€ç»ˆç”Ÿæˆçš„å®Œæ•´æ–‡ä»¶åˆ—è¡¨ï¼š")
    all_files = sorted(os.listdir(save_dir))
    for file in all_files:
        file_size = os.path.getsize(os.path.join(save_dir, file)) / 1024**3
        print(f"  - {file} (å¤§å°ï¼š{file_size:.5f} GB)")

    # metrics = main(train_loader, test_loader, model=model, sample_area=sample_area
    #                   , num_added=num_added, original_vocab_size=original_vocab_size
    #                   , tokenizer=tokenizer, loss_weights=loss_weights)
    # save_metrics = pd.concat([pd.DataFrame(metrics['train']), pd.DataFrame(metrics['val'])], axis=1)
    # save_metrics.to_csv("G:/éƒ¨ç½²ç›¸å…³è®ºæ–‡/LLMèˆ¹èˆ¶è½¨è¿¹é¢„æµ‹/ç»“æœ/STP2LLM_GPT2â€”3.csv")
    
    
    save_dir = "/root/autodl-tmp/llm4stp_qwen/model_qwen2_trained"
    os.makedirs(save_dir, exist_ok=True)
    
    model.llm_config.save_pretrained(save_dir)
    
    print("12")
    
    llm_model = model.llm_model.to(torch.bfloat16)
    
    # æ ¸å¿ƒï¼šå³ä½¿å¼€å¯safe_serializationä¹Ÿèƒ½ä¿å­˜
    llm_model.save_pretrained(
        save_dir,
        safe_serialization=True,  # æ¢å¤safetensorsï¼ˆå¯é€‰ï¼‰
        # max_shard_size="10GB",    # åˆ†ç‰‡ï¼ˆå¯é€‰ï¼‰
        temp_dir=temp_dir         # æ˜¾å¼æŒ‡å®šä¸´æ—¶ç›®å½•ï¼ˆåŒé‡ä¿éšœï¼‰
    )

    print("13")
    llm_model.save_pretrained(
        save_dir,
        safe_serialization=False,  # ä¿å­˜ä¸ºptæ ¼å¼ï¼ˆå¦‚éœ€safetensorsåˆ™è®¾ä¸ºTrueï¼‰
        # max_shard_size="10GB",      # æ¯ä¸ªåˆ†ç‰‡æœ€å¤§2GBï¼ˆæŒ‰éœ€è°ƒæ•´ï¼š1GB/5GB/10GBï¼‰
        save_function=torch.save   # æ˜¾å¼æŒ‡å®šä¿å­˜å‡½æ•°
    )
    
    tokenizer = model.tokenizer

    tokenizer.save_pretrained(save_dir)
